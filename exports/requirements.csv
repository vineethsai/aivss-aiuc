RequirementID,Principle,RequirementTitle,FullRequirement,Application,Frequency,Capabilities,AIVSS_Primary,AIVSS_Secondary,ASI_ID,ASI_Title,Bridge_Used,Confidence,Rationale,Review_Priority,Review_Checks_Failed,Review_Change_Flag,Review_Change_Notes
A001,Data & Privacy,A001: Establish input data policy,"Establish and communicate AI input data policies covering how customer data is used for model training, inference processing, data retention periods, and customer data rights",Mandatory,Every 12 months,Universal,Agent Memory & Context Manipulation,Agent Untraceability,,,N,High,"Input data policies govern how data enters agent memory and context, directly addressing manipulation risks. Policies also support audit.",OK,,,
A002,Data & Privacy,A002: Establish output data policy,"Establish AI output ownership, usage, opt-out and deletion policies to customers and communicate these policies",Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,High,"Output data policies establish ownership and accountability for AI-generated content, directly supporting traceability.",OK,,,
A003,Data & Privacy,A003: Limit AI agent data collection,Implement safeguards to limit AI agent data access to task-relevant information based on user roles and context,Mandatory,Every 12 months,Universal,Agent Memory & Context Manipulation,,ASI06,Memory & Context Poisoning,N,High,Detected cues: context. ASI label inferred: ASI06 (Memory & Context Poisoning). Mapped primarily to AIVSS core risk 'Agent Memory & Context Manipulation' as the most direct agentic failure mode mitigated.,OK,,,
A004,Data & Privacy,A004: Protect IP & trade secrets,Implement safeguards or technical controls to prevent AI systems from leaking company intellectual property or confidential information,Mandatory,Every 12 months,Universal,Agent Access Control Violation,,,,N,Medium,Mapped primarily to AIVSS core risk 'Agent Access Control Violation' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
A005,Data & Privacy,A005: Prevent cross-customer data exposure,Implement safeguards to prevent cross-customer data exposure when combining customer data from multiple sources,Mandatory,Every 12 months,Universal,Agent Access Control Violation,Agent Memory & Context Manipulation,,,N,Medium,"Detected cues: cross-customer, rce. Mapped primarily to AIVSS core risk 'Agent Access Control Violation' as the most direct agentic failure mode mitigated. Secondary impact likely also reduces: Agent Memory & Context Manipulation.",OK,,,
A006,Data & Privacy,A006: Prevent PII leakage,Establish safeguards to prevent personal data leakage through AI outputs,Mandatory,Every 12 months,Universal,Agent Access Control Violation,Agent Memory & Context Manipulation,,,N,High,"PII guardrails enforce data access boundaries, preventing unauthorized access to sensitive information.",OK,,,
A007,Data & Privacy,A007: Prevent IP violations,"Implement safeguards and technical controls to prevent AI outputs from violating copyrights, trademarks, or other third-party intellectual property rights",Mandatory,Every 12 months,"Text-generation, Voice-generation, Image-generation",Agent Supply Chain & Dependency Risk,Agent Untraceability,ASI04,Agentic Supply Chain Vulnerabilities,N,High,IP violation prevention addresses risks from training data and model dependencies containing copyrighted content. Content provenance tracking supports traceability.,OK,,,
B001,Security,B001: Third-party testing of adversarial robustness,Implement adversarial testing program to validate system resilience against adversarial inputs and prompt injection attempts in line with adversarial threat taxonomy,Mandatory,Every 3 months,Universal,Agent Goal & Instruction Manipulation,Agent Untraceability,ASI01,Agent Goal Hijack,N,High,Adversarial testing validates resilience against prompt injection and goal hijacking attacks. Testing records support traceability.,OK,,,
B002,Security,B002: Detect adversarial input,Implement monitoring capabilities to detect and respond to adversarial inputs and prompt injection attempts,Optional,Every 3 months,Universal,Agent Goal & Instruction Manipulation,,ASI01,Agent Goal Hijack,N,Medium,Detected cues: prompt injection. ASI label inferred: ASI01 (Agent Goal Hijack). Mapped primarily to AIVSS core risk 'Agent Goal & Instruction Manipulation' as the most direct agentic failure mode mitigated.,OK,,,
B003,Security,B003: Manage public release of technical details,Implement controls to prevent over-disclosure of technical information about AI systems and organizational details that could enable adversarial targeting,Optional,Every 12 months,Universal,Agent Supply Chain & Dependency Risk,Agent Goal & Instruction Manipulation,ASI04,Agentic Supply Chain Vulnerabilities,N,High,Controlling technical disclosure prevents attackers from learning system architecture to craft targeted attacks. This protects the supply chain by limiting attack surface knowledge. Goal manipulation is secondary as disclosure enables prompt injection attacks.,OK,,,
B004,Security,B004: Prevent AI endpoint scraping,Implement safeguards to prevent probing or scraping of external AI endpoints,Mandatory,Every 12 months,Universal,Agentic AI Tool Misuse,Agent Access Control Violation,,,N,High,Preventing endpoint scraping stops misuse of AI APIs as attack tools. Access controls provide additional protection.,OK,,,
B005,Security,B005: Implement real-time input filtering,Implement real-time input filtering using automated moderation tools,Optional,Every 12 months,"Text-generation, Voice-generation, Image-generation",Agent Goal & Instruction Manipulation,Agent Untraceability,,,N,High,Input filtering primarily mitigates prompt injection and goal hijacking attacks. Logging of filtered inputs supports traceability.,OK,,Y,Manual review: input filtering mitigates prompt injection/goal hijack; documentation/audit secondary.
B006,Security,B006: Prevent unauthorized AI agent actions,Implement safeguards to limit AI agent system access based on context and declared objectives,Mandatory,Every 12 months,Automation,Agent Memory & Context Manipulation,,ASI06,Memory & Context Poisoning,N,Medium,Detected cues: context. ASI label inferred: ASI06 (Memory & Context Poisoning). Mapped primarily to AIVSS core risk 'Agent Memory & Context Manipulation' as the most direct agentic failure mode mitigated.,OK,,,
B007,Security,B007: Enforce user access privileges to AI systems,Establish and maintain user access controls and admin privileges for AI systems in line with policy,Mandatory,Every 3 months,Universal,Agent Access Control Violation,Agent Untraceability,ASI03,Identity & Privilege Abuse,N,High,User access controls and privilege management directly prevent unauthorized access to AI systems. Audit trails support traceability of access changes.,OK,,,
B008,Security,B008: Protect model deployment environment,"Implement security measures for AI model deployment environments including encryption, access controls and authorization",Mandatory,Every 12 months,Universal,Agent Access Control Violation,,ASI03,Identity & Privilege Abuse,N,Medium,ASI label inferred: ASI03 (Identity & Privilege Abuse). Mapped primarily to AIVSS core risk 'Agent Access Control Violation' as the most direct agentic failure mode mitigated.,OK,,,
B009,Security,B009: Limit output over-exposure,Implement output limitations and obfuscation techniques to safeguard against information leakage,Mandatory,Every 12 months,"Text-generation, Voice-generation",Agent Memory & Context Manipulation,Agent Untraceability,,,N,High,Output obfuscation prevents cross-user/session data leakage through agent memory. Audit trails provide traceability.,OK,,Y,Manual review: output obfuscation to prevent leakage aligns with memory/context (cross-user/session leakage); user notices/audit secondary.
C001,Safety,C001: Define AI risk taxonomy,"Establish a risk taxonomy that categorizes risks within harmful, out-of-scope, and hallucinated outputs, tool calls, and other risks based on application-specific usage",Mandatory,Every 3 months,Universal,Agent Goal & Instruction Manipulation,Agentic AI Tool Misuse,,,N,High,"Risk taxonomy defines boundaries for acceptable agent behavior, helping detect goal manipulation. Tool call risks are secondary.",OK,,,
C002,Safety,C002: Conduct pre-deployment testing,Conduct internal testing of AI systems prior to deployment across risk categories for system changes requiring formal review or approval,Mandatory,Every 12 months,Universal,Agent Supply Chain & Dependency Risk,Agent Untraceability,,,N,High,"Pre-deployment testing validates system integrity before deployment, addressing supply chain risks. Documentation supports traceability.",OK,,,
C003,Safety,C003: Prevent harmful outputs,"Implement safeguards or technical controls to prevent harmful outputs including distressed outputs, angry responses, high-risk advice, offensive content, bias, and deception",Mandatory,Every 12 months,"Text-generation, Voice-generation, Image-generation",Agent Goal & Instruction Manipulation,Insecure Agent Critical Systems Interaction,,,N,High,Harmful output prevention directly addresses goal manipulation attacks that attempt to elicit dangerous content. Critical systems interaction is secondary.,OK,,Y,Manual review: harmful outputs often induced by prompt/goal manipulation; critical interaction secondary when outputs drive unsafe actions.
C004,Safety,C004: Prevent out-of-scope outputs,"Implement safeguards or technical controls to prevent out-of-scope outputs (e.g. political discussion, healthcare advice)",Mandatory,Every 12 months,"Text-generation, Voice-generation",Agent Goal & Instruction Manipulation,Agent Untraceability,ASI01,Agent Goal Hijack,N,High,"Preventing out-of-scope outputs constrains agent behavior within defined boundaries, directly addressing goal manipulation where attackers try to elicit unauthorized content.",OK,LowConfidence; WeakSignals,,
C005,Safety,C005: Prevent customer-defined high risk outputs,Implement safeguards or technical controls to prevent additional high risk outputs as defined in risk taxonomy,Mandatory,Every 12 months,Universal,Agent Goal & Instruction Manipulation,Agent Cascading Failures,ASI01,Agent Goal Hijack,N,High,Preventing high-risk outputs as defined in risk taxonomy constrains agent goal execution. Cascading failures are secondary when harmful outputs propagate.,OK,LowConfidence; WeakSignals,,
C006,Safety,C006: Prevent output vulnerabilities,Implement safeguards to prevent security vulnerabilities in outputs from impacting users,Mandatory,Every 3 months,Universal,Insecure Agent Critical Systems Interaction,Agent Cascading Failures,ASI05,Unexpected Code Execution (RCE),N,High,"Preventing security vulnerabilities in outputs (e.g., code injection, XSS) protects critical systems from agent-generated exploits. Cascading failures are secondary.",OK,LowConfidence; WeakSignals,,
C007,Safety,C007: Flag high risk outputs,Implement an alerting system that flags high-risk outputs for human review,Optional,Every 12 months,Universal,Agent Cascading Failures,Agent Untraceability,ASI08,Cascading Failures,N,High,"Flagging high-risk outputs for human review prevents harmful agent outputs from propagating into downstream systems or decisions, directly addressing cascading failures. Audit trails support traceability.",OK,,,
C008,Safety,C008: Monitor AI risk categories,Implement monitoring of AI systems across risk categories,Optional,Every 12 months,Universal,Agent Untraceability,Agent Goal & Instruction Manipulation,ASI10,Rogue Agents,N,High,"Monitoring AI systems across risk categories provides visibility into agent behavior, directly addressing untraceability. Detecting behavioral drift also helps identify goal manipulation.",OK,,,
C009,Safety,C009: Enable real-time feedback and intervention,Implement mechanisms to enable real-time user feedback collection and intervention mechanisms,Optional,Every 3 months,Universal,Agent Cascading Failures,Agent Goal & Instruction Manipulation,ASI08,Cascading Failures,N,High,Real-time intervention mechanisms (stop/pause/redirect) allow users to halt harmful agent behavior before it cascades. Goal manipulation is secondary as interventions can stop hijacked agents.,OK,,Y,Manual review: real-time user intervention/stop mechanisms contain cascade/goal-hijack impacts.
C010,Safety,C010: Third-party testing for harmful outputs,"Appoint expert third parties to evaluate system robustness to harmful outputs including distressed outputs, angry responses, high-risk advice, offensive content, bias, and deception at least every 3 months",Mandatory,Every 3 months,"Text-generation, Voice-generation, Image-generation",Agent Goal & Instruction Manipulation,Agent Untraceability,ASI01,Agent Goal Hijack,N,High,Third-party testing for harmful outputs validates defenses against goal manipulation attacks that produce dangerous content. Testing records support traceability.,OK,,,
C011,Safety,C011: Third-party testing for out-of-scope outputs,"Appoint expert third parties to evaluate system robustness to out-of-scope outputs at least every 3 months (e.g. political discussion, healthcare advice)",Mandatory,Every 3 months,"Text-generation, Voice-generation",Agent Goal & Instruction Manipulation,Agent Untraceability,ASI01,Agent Goal Hijack,N,High,Third-party testing for out-of-scope outputs validates defenses against goal manipulation that elicits unauthorized content. Testing records support traceability.,OK,,,
C012,Safety,C012: Third-party testing for customer-defined risk,Appoint expert third-parties to evaluate system robustness to additional high-risk outputs as defined in risk taxonomy at least every 3 months,Mandatory,Every 3 months,Universal,Agent Supply Chain & Dependency Risk,,ASI04,Agentic Supply Chain Vulnerabilities,N,Medium,"Detected cues: eval, testing. ASI label inferred: ASI04 (Agentic Supply Chain Vulnerabilities). Mapped primarily to AIVSS core risk 'Agent Supply Chain & Dependency Risk' as the most direct agentic failure mode mitigated.",OK,,,
D001,Reliability,D001: Prevent hallucinated outputs,Implement safeguards or technical controls to prevent hallucinated outputs,Mandatory,Every 12 months,"Text-generation, Voice-generation",Agent Cascading Failures,Agent Untraceability,,,N,High,"Hallucination prevention stops false information from propagating through agent chains, preventing cascading failures.",OK,,Y,Manual review: hallucination prevention reduces hallucination propagation/cascades; citations/traceability secondary.
D002,Reliability,D002: Third-party testing for hallucinations,Appoint expert third-parties to evaluate hallucinated outputs at least every 3 months,Mandatory,Every 3 months,"Text-generation, Voice-generation",Agent Cascading Failures,Agent Untraceability,ASI08,Cascading Failures,N,High,Third-party hallucination testing detects false outputs before they cascade into user decisions. Testing records support traceability.,OK,,,
D003,Reliability,D003: Restrict unsafe tool calls,"Implement safeguards or technical controls to prevent tool calls in AI systems from executing unauthorized actions, accessing restricted information, or making decisions beyond their intended scope",Mandatory,Every 12 months,Automation,Agentic AI Tool Misuse,Agent Access Control Violation,ASI02,Tool Abuse,N,High,Safeguards preventing unauthorized tool calls directly address agentic tool misuse. Access controls are secondary.,OK,,,
D004,Reliability,D004: Third-party testing of tool calls,"Appoint expert third-parties to evaluate tool calls in AI systems, including executing unauthorized actions, accessing restricted information, or making decisions beyond their intended scope at least every 3 months",Mandatory,Every 3 months,Automation,Agentic AI Tool Misuse,Agent Access Control Violation,ASI02,Tool Abuse,N,High,Third-party testing of tool calls validates defenses against unauthorized tool execution. Access control testing is secondary.,OK,,,
E001,Accountability,E001: AI failure plan for security breaches,"Document AI failure plan for AI privacy and security breaches assigning accountable owners and establishing notification and remediation with third-party support as needed (e.g. legal, PR, insurers)",Mandatory,Every 12 months,Universal,Agent Cascading Failures,Agent Untraceability,ASI08,Cascading Failures,N,High,AI failure plans for security breaches establish response procedures to contain incidents before they cascade. Accountability assignments support traceability.,OK,,,
E002,Accountability,E002: AI failure plan for harmful outputs,"Document AI failure plan for harmful AI outputs that cause significant customer harm assigning accountable owners and establishing remediation with third-party support as needed (e.g. legal, PR, insurers)",Mandatory,Every 12 months,"Text-generation, Voice-generation, Image-generation",Agent Goal & Instruction Manipulation,Agent Cascading Failures,ASI01,Agent Goal Hijack,N,High,Failure plans for harmful outputs address the consequences of goal manipulation attacks that produce dangerous content. Cascade prevention is secondary.,OK,,,
E003,Accountability,E003: AI failure plan for hallucinations,"Document AI failure plan for hallucinated AI outputs that cause substantial customer financial loss assigning accountable owners and establishing remediation with third-party support as needed (e.g. legal, PR, insurers)",Mandatory,Every 12 months,"Text-generation, Voice-generation",Agent Cascading Failures,Agent Untraceability,ASI08,Cascading Failures,N,High,Failure plans for hallucinations address scenarios where false agent outputs cascade into financial harm. Accountability supports traceability.,OK,,,
E004,Accountability,E004: Assign accountability,"Document which AI system changes across the development & deployment lifecycle require formal review or approval, assign a lead accountable for each, and document their approval with supporting evidence",Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Detected cues: approval. Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E005,Accountability,E005: Assess cloud vs on-prem processing,"Establish criteria for selecting cloud provider, and circumstances for on-premises processing considering data sensitivity, regulatory requirements, security controls, and operational needs",Mandatory,Every 12 months,Universal,Agent Supply Chain & Dependency Risk,Agent Access Control Violation,ASI04,Agentic Supply Chain Vulnerabilities,N,High,Cloud vs on-prem decisions directly impact supply chain dependencies and third-party risk exposure. Access control considerations are secondary.,OK,,,
E006,Accountability,E006: Conduct vendor due diligence,"Establish AI vendor due diligence processes for foundation and upstream model providers covering data handling, PII controls, security and compliance",Mandatory,Every 12 months,Universal,Agent Supply Chain & Dependency Risk,Agent Untraceability,,,N,Medium,Detected cues: pii. Mapped primarily to AIVSS core risk 'Agent Supply Chain & Dependency Risk' as the most direct agentic failure mode mitigated. Secondary impact likely also reduces: Agent Untraceability.,OK,,,
E007,Accountability,E007: [Retired] Document system change approvals,Merged with E004 - see changelog (Q1 2026 update),Optional,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Detected cues: approval. Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E008,Accountability,E008: Review internal processes,Establish regular internal reviews of key processes and document review records and approvals,Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Detected cues: approval. Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E009,Accountability,E009: Monitor third-party access,Implement systems to monitor third party access,Optional,Every 12 months,Universal,Agent Supply Chain & Dependency Risk,Agent Untraceability,ASI04,Agentic Supply Chain Vulnerabilities,N,Medium,ASI label inferred: ASI04 (Agentic Supply Chain Vulnerabilities). Mapped primarily to AIVSS core risk 'Agent Supply Chain & Dependency Risk' as the most direct agentic failure mode mitigated. Secondary impact likely also reduces: Agent Untraceability.,OK,,,
E010,Accountability,E010: Establish AI acceptable use policy,Establish and implement an AI acceptable use policy,Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E011,Accountability,E011: Record processing locations,Document AI data processing locations,Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E012,Accountability,E012: Document regulatory compliance,"Document applicable AI laws and standards, required data protections, and strategies for compliance",Mandatory,Every 6 months,Universal,Agent Untraceability,,,,N,Medium,Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,OK,,,
E013,Accountability,E013: Implement quality management system,Establish a quality management system for AI systems proportionate to the size of the organization,Optional,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Detected cues: quality management. Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E014,Accountability,E014: Share transparency reports,Merged with E017 - see changelog (Q1 2026 update),Optional,Every 12 months,Universal,Agent Untraceability,,,,N,High,Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E015,Accountability,E015: Log model activity,"Maintain logs of AI system processes, actions, and model outputs where permitted to support incident investigation, auditing, and explanation of AI system behavior",Mandatory,Every 12 months,Universal,Agent Untraceability,,ASI10,Rogue Agents,N,Medium,Detected cues: audit. ASI label inferred: ASI10 (Rogue Agents). Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,OK,,,
E016,Accountability,E016: Implement AI disclosure mechanisms,Implement clear disclosure mechanisms to inform users when they are interacting with AI systems rather than humans,Mandatory,Every 12 months,Universal,Agent Untraceability,,,,N,Medium,Detected cues: disclosure. Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,P2,WeakSignals,,
E017,Accountability,E017: Document system transparency policy,"Establish a system transparency policy and maintain a repository of model cards, datasheets, and interpretability reports for major systems",Optional,Every 12 months,Universal,Agent Untraceability,,,,N,High,Mapped primarily to AIVSS core risk 'Agent Untraceability' as the most direct agentic failure mode mitigated.,OK,,,
F001,Society,F001: Prevent AI cyber misuse,Implement or document guardrails to prevent AI-enabled misuse for cyber attacks and exploitation,Mandatory,Every 12 months,"Text-generation, Automation, Voice-generation",Agentic AI Tool Misuse,Agent Access Control Violation,,,N,High,Cyber misuse guardrails prevent agents from using tools for malicious purposes. Access controls provide defense in depth.,OK,,Y,Manual review: cyber misuse guardrails primarily address tool misuse; access control secondary (least privilege/abuse).
F002,Society,F002: Prevent catastrophic misuse,Implement or document guardrails to prevent AI-enabled catastrophic system misuse (chemical / bio / radio / nuclear),Mandatory,Every 12 months,"Text-generation, Voice-generation, Image-generation",Insecure Agent Critical Systems Interaction,Agentic AI Tool Misuse,,,N,High,Catastrophic misuse prevention focuses on protecting critical systems from agent-induced harm. Tool misuse controls are secondary.,OK,,Y,Manual review: catastrophic misuse tied to critical/physical harm contexts; tool misuse secondary.
